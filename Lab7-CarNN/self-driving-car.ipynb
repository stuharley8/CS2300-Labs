{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OnhKCxw_so3u"
   },
   "source": [
    "# Self-Driving Introduction\n",
    "Derek Riley\n",
    "\n",
    "This notebook allows a user to train a Deep Neural Network to \"learn\" how to drive a simulated car.  This was built by adapting a Udacity tutorial https://www.udacity.com/course/self-driving-car-engineer-nanodegree--nd013\n",
    "\n",
    "Lets start with a few imports..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 383,
     "status": "ok",
     "timestamp": 1552004522046,
     "user": {
      "displayName": "Derek Riley",
      "photoUrl": "https://lh3.googleusercontent.com/-aBlfyFPL7aY/AAAAAAAAAAI/AAAAAAAACeg/qnKtAgMG0QE/s64/photo.jpg",
      "userId": "08414629839343702638"
     },
     "user_tz": 360
    },
    "id": "2pXd4Xwz5xJz",
    "outputId": "accc5de2-85f8-4301-c164-3ba513035568"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Flatten, Dense, Lambda, Conv2D, Cropping2D, Dropout, ELU\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint, Callback\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "import io\n",
    "# %load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a function to create the Keras DNN.  Note that minor adjustments to this model can severly break it.  Please avoid the temptation to play around this until you have a solid understanding of the model and what constraints exist.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def commaAiModelPrime(time_len=1):\n",
    "    \"\"\"\n",
    "    Creates comma.ai enhanced autonomous car  model\n",
    "    Replaced dropout with regularization\n",
    "    Added 3 additional convolution layers\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Lambda(lambda x: (x / 255.0) - 0.5, input_shape=(160,320,3)))\n",
    "    model.add(Cropping2D(cropping=((50,20), (0,0))))\n",
    "\n",
    "    # Add three 5x5 convolution layers (output depth 64, and 64)\n",
    "    model.add(Conv2D(16, (8, 8), strides=(4, 4), padding=\"same\", kernel_regularizer=l2(0.001)))\n",
    "    model.add(ELU())\n",
    "    model.add(Conv2D(32, (5, 5), strides=(2, 2), padding=\"same\", kernel_regularizer=l2(0.001)))\n",
    "    model.add(ELU())\n",
    "    model.add(Conv2D(48, (5, 5), strides=(2, 2), padding=\"same\", kernel_regularizer=l2(0.001)))\n",
    "    model.add(ELU())\n",
    "\n",
    "    # Add two 3x3 convolution layers (output depth 64, and 64)\n",
    "    model.add(Conv2D(64, (3, 3), padding='valid', kernel_regularizer=l2(0.001)))\n",
    "    model.add(ELU())\n",
    "    model.add(Conv2D(64, (3, 3), padding='valid', kernel_regularizer=l2(0.001)))\n",
    "    model.add(ELU())\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # model.add(Dropout(.2))\n",
    "    model.add(Dense(100, kernel_regularizer=l2(0.001)))\n",
    "    model.add(ELU())\n",
    "\n",
    "    # model.add(Dropout(0.50))\n",
    "    model.add(Dense(50, kernel_regularizer=l2(0.001)))\n",
    "    model.add(ELU())\n",
    "\n",
    "    # model.add(Dropout(0.50))\n",
    "    model.add(Dense(10, kernel_regularizer=l2(0.001)))\n",
    "    model.add(ELU())\n",
    "\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.compile(optimizer=Adam(lr=1e-4), loss='mse')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions load the driving data and images.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V7W6uI_X6eth"
   },
   "outputs": [],
   "source": [
    "def getDrivingLogs(path, skipHeader=False):\n",
    "    \"\"\"\n",
    "    Returns the lines from a driving log with base directory `dataPath`.\n",
    "    If the file include headers, pass `skipHeader=True`.\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    with open(path + '/driving_log.csv') as csvFile:\n",
    "        reader = csv.reader(csvFile)\n",
    "        if skipHeader:\n",
    "            next(reader, None)\n",
    "        for line in reader:\n",
    "            lines.append(line)\n",
    "    return lines\n",
    "\n",
    "\n",
    "def getImages(path):\n",
    "    \"\"\"\n",
    "    Get all training images on the path `dataPath`.\n",
    "    Returns `([centerPaths], [leftPath], [rightPath], [measurement])`\n",
    "    \"\"\"\n",
    "    directories = [x[0] for x in os.walk(path)]\n",
    "    dataDirectories = list(filter(lambda directory: os.path.isfile(\n",
    "        directory + '/driving_log.csv'), directories))\n",
    "    print(dataDirectories)\n",
    "    centerTotal = []\n",
    "    leftTotal = []\n",
    "    rightTotal = []\n",
    "    measurementTotal = []\n",
    "    for directory in dataDirectories:\n",
    "        lines = getDrivingLogs(directory, skipHeader=True)\n",
    "        center = []\n",
    "        left = []\n",
    "        right = []\n",
    "        measurements = []\n",
    "        for line in lines:\n",
    "            measurements.append(float(line[3]))\n",
    "            center.append(directory + '/' + line[0].strip())\n",
    "            left.append(directory + '/' + line[1].strip())\n",
    "            right.append(directory + '/' + line[2].strip())\n",
    "        centerTotal.extend(center)\n",
    "        leftTotal.extend(left)\n",
    "        rightTotal.extend(right)\n",
    "        measurementTotal.extend(measurements)\n",
    "\n",
    "    return (centerTotal, leftTotal, rightTotal, measurementTotal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function combines images to prepare them to feed into the network.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineCenterLeftRightImages(center, left, right, measurement, correction):\n",
    "    \"\"\"\n",
    "    Combine the image paths from `center`, `left` and `right` using the correction factor `correction`\n",
    "    Returns ([imagePaths], [measurements])\n",
    "    \"\"\"\n",
    "    imagePaths = []\n",
    "    imagePaths.extend(center)\n",
    "    imagePaths.extend(left)\n",
    "    imagePaths.extend(right)\n",
    "    measurements = []\n",
    "    measurements.extend(measurement)\n",
    "    measurements.extend([x + correction for x in measurement])\n",
    "    measurements.extend([x - correction for x in measurement])\n",
    "    return (imagePaths, measurements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generator provides data to train and validate the neural network.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(samples, batch_size=32, flip = True):\n",
    "    \"\"\"\n",
    "    Generate the required images and measurments for training/\n",
    "    `samples` is a list of pairs (`imagePath`, `measurement`).\n",
    "    \"\"\"\n",
    "    num_samples = len(samples)\n",
    "    while 1: # Loop forever so the generator never terminates\n",
    "        samples = sklearn.utils.shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "\n",
    "            images = []\n",
    "            angles = []\n",
    "            for imagePath, measurement in batch_samples:\n",
    "                originalImage = cv2.imread(imagePath)\n",
    "                image = cv2.cvtColor(originalImage, cv2.COLOR_BGR2RGB)\n",
    "                images.append(image)\n",
    "                angles.append(measurement)\n",
    "                # Flipping\n",
    "                if(flip):\n",
    "                    images.append(cv2.flip(image,1))\n",
    "                    angles.append(measurement*-1.0)\n",
    "\n",
    "            # trim image to only see section with road\n",
    "            inputs = np.array(images)\n",
    "            outputs = np.array(angles)\n",
    "            yield sklearn.utils.shuffle(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the \"main\" functionality of this training.  You will need to adjust the data paths, and you will adjust the batch size and number of epochs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1305
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19806,
     "status": "error",
     "timestamp": 1552004541518,
     "user": {
      "displayName": "Derek Riley",
      "photoUrl": "https://lh3.googleusercontent.com/-aBlfyFPL7aY/AAAAAAAAAAI/AAAAAAAACeg/qnKtAgMG0QE/s64/photo.jpg",
      "userId": "08414629839343702638"
     },
     "user_tz": 360
    },
    "id": "6ed9H11H6obI",
    "outputId": "f35b7e6b-cdc6-4071-feaa-6527bd2d4cb0",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "my_home = \"/home/harleys/CS-2300/lab8/SelfDrivingSim\"\n",
    "my_data = \"/data/cs2300/L8\"\n",
    "my_batch_size = 32\n",
    "my_epochs = 5\n",
    "\n",
    "# Reading images locations.\n",
    "centerPaths, leftPaths, rightPaths, measurements = getImages(my_data)\n",
    "imagePaths, measurements = combineCenterLeftRightImages(\n",
    "    centerPaths, leftPaths, rightPaths, measurements, 0.2)\n",
    "print('Total Images: {}'.format(len(imagePaths)))\n",
    "\n",
    "# Splitting samples and creating generators.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "samples = list(zip(imagePaths, measurements))\n",
    "train_samples, validation_samples = train_test_split(samples, test_size=0.2)\n",
    "\n",
    "print('Train samples: {}'.format(len(train_samples)))\n",
    "print('Validation samples: {}'.format(len(validation_samples)))\n",
    "\n",
    "train_generator = generator(train_samples, batch_size=my_batch_size)\n",
    "validation_generator = generator(validation_samples, batch_size=my_batch_size)\n",
    "\n",
    "# Model creation\n",
    "model = commaAiModelPrime()\n",
    "\n",
    "# Train the model\n",
    "history_object = model.fit_generator(train_generator, steps_per_epoch= \\\n",
    "    len(train_samples)/my_batch_size, validation_data=validation_generator, \\\n",
    "    validation_steps=len(validation_samples)/my_batch_size, epochs=my_epochs, verbose=1)\n",
    "\n",
    "model.save(my_home + '/model_commaAiModelPrime_e5.h5')\n",
    "\n",
    "print(history_object.history.keys())\n",
    "print('Loss')\n",
    "print(history_object.history['loss'])\n",
    "print('Validation Loss')\n",
    "print(history_object.history['val_loss'])\n",
    "\n",
    "plt.plot(history_object.history['loss'])\n",
    "plt.plot(history_object.history['val_loss'])\n",
    "plt.title('model mean squared error loss')\n",
    "plt.ylabel('mean squared error loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training set', 'validation set'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1305
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19806,
     "status": "error",
     "timestamp": 1552004541518,
     "user": {
      "displayName": "Derek Riley",
      "photoUrl": "https://lh3.googleusercontent.com/-aBlfyFPL7aY/AAAAAAAAAAI/AAAAAAAACeg/qnKtAgMG0QE/s64/photo.jpg",
      "userId": "08414629839343702638"
     },
     "user_tz": 360
    },
    "id": "6ed9H11H6obI",
    "outputId": "f35b7e6b-cdc6-4071-feaa-6527bd2d4cb0",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    my_home = \"/home/harleys/CS-2300/lab8/SelfDrivingSim\"\n",
    "    my_data = \"/home/harleys/CS-2300/lab8\"\n",
    "    my_batch_size = 32\n",
    "    my_epochs = 20\n",
    "\n",
    "    # Reading images locations.\n",
    "    centerPaths, leftPaths, rightPaths, measurements = getImages(my_data)\n",
    "    imagePaths, measurements = combineCenterLeftRightImages(\n",
    "        centerPaths, leftPaths, rightPaths, measurements, 0.2)\n",
    "    print('Total Images: {}'.format(len(imagePaths)))\n",
    "\n",
    "    # Splitting samples and creating generators.\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    samples = list(zip(imagePaths, measurements))\n",
    "    train_samples, validation_samples = train_test_split(samples, test_size=0.2)\n",
    "\n",
    "    print('Train samples: {}'.format(len(train_samples)))\n",
    "    print('Validation samples: {}'.format(len(validation_samples)))\n",
    "\n",
    "    train_generator = generator(train_samples, batch_size=my_batch_size)\n",
    "    validation_generator = generator(validation_samples, batch_size=my_batch_size)\n",
    "\n",
    "    # Model creation\n",
    "    model = commaAiModelPrime()\n",
    "\n",
    "    # Train the model\n",
    "    history_object = model.fit_generator(train_generator, steps_per_epoch= \\\n",
    "        len(train_samples)/my_batch_size, validation_data=validation_generator, \\\n",
    "        validation_steps=len(validation_samples)/my_batch_size, epochs=my_epochs, verbose=1)\n",
    "\n",
    "    model.save(my_home + '/final_model.h5')\n",
    "\n",
    "    print(history_object.history.keys())\n",
    "    print('Loss')\n",
    "    print(history_object.history['loss'])\n",
    "    print('Validation Loss')\n",
    "    print(history_object.history['val_loss'])\n",
    "\n",
    "    plt.plot(history_object.history['loss'])\n",
    "    plt.plot(history_object.history['val_loss'])\n",
    "    plt.title('model mean squared error loss')\n",
    "    plt.ylabel('mean squared error loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['training set', 'validation set'], loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %lprun -f create_model create_model()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of self-driving-car.ipynb",
   "provenance": [
    {
     "file_id": "1w7YtwC3LLhc6RMHODUyk-ILSvYAoMlSG",
     "timestamp": 1552073233603
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
