{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Search Terms 2.0 NumPy\n",
    "## Stuart Harley\n",
    "\n",
    "**Introduction**: In lab 3 we created a list of the most popular search terms (tokens) for a given set of search queries.  As people are imperfect, they often misspell search terms, so we are using a spell checking library to remove and interpret misspellings to find the actual most popular terms (and not just the most popular and consistently spelled terms). The search terms come from Direct Supply's DSSI eProcurement system.\n",
    "\n",
    "In this lab, we are doing the same thing but we are using numpy and pandas to manipulate the data. This specific notebook uses Numpy.\n",
    "\n",
    "**Learning Outcomes**:\n",
    "- Data importing with Numpy and Pandas\n",
    "- Cleaning data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "import pattern.en\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function splits tokens at %20's and spaces**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_token(string):\n",
    "    return re.split('%20|\\s', string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to clean strings**: Removes surrounding parenthesis from a string and sets the strings to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    if(string.startswith('\"')):\n",
    "        string = string[1:]\n",
    "    if(string.endswith('\"')):\n",
    "        string = string[:-1]\n",
    "    string = string.lower()\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing csv data into a 1D numpy array**: I ignore the cases where there is a second word separated by a comma in the csv file since it happens only a small percent of the time. I also ignore the header line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bytes in original_tokens: 96\n",
      "CPU times: user 2.19 s, sys: 307 ms, total: 2.5 s\n",
      "Wall time: 2.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "original_tokens = np.genfromtxt('/home/harleys/searchTerms.csv', delimiter=',', dtype=str, usecols=(0), skip_header=1)\n",
    "print(\"bytes in original_tokens: \" + str(sys.getsizeof(original_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creates a 1D array of all tokens**: (splits the tokens at any %20's and spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bytes in split_tokens: 199758528\n",
      "CPU times: user 2.56 s, sys: 164 ms, total: 2.72 s\n",
      "Wall time: 2.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "split_tokens = np.array([elem for singleList in list(map(split_token, original_tokens)) for elem in singleList])\n",
    "print(\"bytes in split_tokens: \" + str(sys.getsizeof(split_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creates a 1D array of cleaned tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bytes in cleaned_tokens: 199758528\n",
      "CPU times: user 904 ms, sys: 163 ms, total: 1.07 s\n",
      "Wall time: 1.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cleaned_tokens = np.array(list(map(clean_str, split_tokens)))\n",
    "print(\"bytes in cleaned_tokens: \" + str(sys.getsizeof(cleaned_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a numpy 2D array consolidating same search words and keeping track of the number of times they occur**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bytes in tokens_count: 16732464\n",
      "CPU times: user 4.03 s, sys: 437 ms, total: 4.47 s\n",
      "Wall time: 665 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "unique, counts = np.unique(cleaned_tokens, return_counts=True)\n",
    "tokens_count = np.asarray((unique, counts))\n",
    "print(\"bytes in tokens_count: \" + str(sys.getsizeof(tokens_count)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating an output csv file from the numpy 2D array of tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('/home/harleys/numpy_all_tokens.csv', tokens_count.T, delimiter=',', fmt=\"%s\", header='SearchToken, Occurances')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example results of tokens and their number of occurances**: In this cell several entries from the beginning and end of the tokens_count numpy 2D array were printed out to illustrate what the csv file looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['' '543']\n",
      " ['\"' '1']\n",
      " ['%' '15']\n",
      " ...\n",
      " ['zymox' '1']\n",
      " ['zyno' '13']\n",
      " ['zyrtec' '352']]\n"
     ]
    }
   ],
   "source": [
    "print(tokens_count.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a new numpy 2D array with only alphabetic tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bytes in alphabetic_tokens_count: 1820056\n",
      "CPU times: user 3.74 s, sys: 380 ms, total: 4.12 s\n",
      "Wall time: 1.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "alphabetic_tokens = np.array(list(filter(lambda token : token.isalpha(), cleaned_tokens)))\n",
    "unique_alphabetic_tokens, counts = np.unique(alphabetic_tokens, return_counts=True)\n",
    "alphabetic_tokens_count = np.asarray((unique_alphabetic_tokens, counts))\n",
    "print(\"bytes in alphabetic_tokens_count: \" + str(sys.getsizeof(alphabetic_tokens_count)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spellchecking the alphabetic tokens and adding the correct spellings to a new 2D array**: The first value is the alphabetic token and the second is the predicted correct spelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bytes in spellchecked_tokens: 1560064\n",
      "CPU times: user 2.8 s, sys: 16.9 ms, total: 2.82 s\n",
      "Wall time: 2.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spell = SpellChecker(distance=1)\n",
    "correct_spelling_tokens = np.array(list(map(lambda token : spell.correction(token), unique_alphabetic_tokens)))\n",
    "spellchecked_tokens = np.array((unique_alphabetic_tokens, correct_spelling_tokens))\n",
    "print(\"bytes in spellchecked_tokens: \" + str(sys.getsizeof(spellchecked_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating an output csv file from the numpy 2D array of tokens and their possible correct spellings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('/home/harleys/numpy_tokens_and_correct_spellings.csv', spellchecked_tokens.T, delimiter=',', fmt=\"%s\", header='SearchToken, PossibleCorrectSpelling')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example results of tokens and their possible correct spellings**: In this cell several entries from the beginning and end of the spellchecked numpy 2D array were printed out to illustrate what the csv file looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a' 'a']\n",
      " ['aa' 'aa']\n",
      " ['aaa' 'aaa']\n",
      " ...\n",
      " ['zymox' 'zymox']\n",
      " ['zyno' 'zeno']\n",
      " ['zyrtec' 'zyrtec']]\n"
     ]
    }
   ],
   "source": [
    "print(spellchecked_tokens.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a final 1D array of unique spell checked tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bytes in unique_correct_spelling_tokens: 648096\n",
      "CPU times: user 4.83 ms, sys: 76 µs, total: 4.91 ms\n",
      "Wall time: 4.23 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "unique_correct_spelling_tokens = np.unique(correct_spelling_tokens)\n",
    "print(\"bytes in unique_correct_spelling_tokens: \" + str(sys.getsizeof(unique_correct_spelling_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a final dictionary of tokens**: In this cell, the final dictionary of spell checked tokens is created. If a token was misspelled and a correct spelling was found, then the number of occurances of the misspelled word is added to the number of occurances of the correctly spelled word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bytes in final_spelled_dict: 295008\n",
      "CPU times: user 13.3 ms, sys: 119 µs, total: 13.4 ms\n",
      "Wall time: 13.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "temp_dict = dict(zip(correct_spelling_tokens, counts))\n",
    "final_spelled_dict = {}\n",
    "for token in unique_correct_spelling_tokens:\n",
    "    final_spelled_dict[token] = 0;\n",
    "for key in temp_dict.keys():\n",
    "    final_spelled_dict[key] += temp_dict[key]\n",
    "print(\"bytes in final_spelled_dict: \" + str(sys.getsizeof(final_spelled_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating an output csv file from the dictionary of correctly spelled tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/harleys/numpy_correctly_spelled_tokens.csv', 'w') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\"SearchToken\", \"Occurances\"])\n",
    "    writer.writeheader()\n",
    "    for key in final_spelled_dict.keys():\n",
    "        file.write(\"%s,%s\\n\"%(key,final_spelled_dict[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "- Conclusion is contained in the Pandas notebook portion of the lab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
