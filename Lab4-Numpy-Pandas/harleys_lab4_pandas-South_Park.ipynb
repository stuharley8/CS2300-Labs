{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Search Terms 2.0 Pandas\n",
    "## Stuart Harley\n",
    "\n",
    "**Introduction**: In this notebook, I am testing my Pandas implementation of the search term lab with a new list of search terms. The search terms come from a script of a South Park episode. \n",
    "\n",
    "**Learning Outcomes**:\n",
    "- Data importing with Numpy and Pandas\n",
    "- Cleaning data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "import pattern.en\n",
    "import csv\n",
    "import time\n",
    "import sys\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "pd.options.mode.chained_assignment = None # hides a SettingWithCopy warning later on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function splits tokens at %20's and spaces**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_token(string):\n",
    "    return re.split('%20|\\s|,', string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing csv data into a pandas dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bytes in original_tokens: 8514555\n",
      "CPU times: user 87.9 ms, sys: 15.9 ms, total: 104 ms\n",
      "Wall time: 104 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "original_tokens = pd.read_csv('/home/harleys/All-seasons.csv', usecols=[\"Line\"], dtype=str)\n",
    "print(\"bytes in original_tokens: \" + str(sys.getsizeof(original_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creates a new dataframe containing all tokens**: (this splits the tokens at spaces, commas, or %20's.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bytes in all_tokens: 59262505\n",
      "CPU times: user 696 ms, sys: 32.1 ms, total: 728 ms\n",
      "Wall time: 727 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokens = original_tokens[\"Line\"].to_numpy()\n",
    "all_tokens = pd.DataFrame([elem for singleList in list(map(split_token, tokens)) for elem in singleList])\n",
    "all_tokens.columns = [\"SearchTerm\"]\n",
    "print(\"bytes in all_tokens: \" + str(sys.getsizeof(all_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adds a column in the dataframe containing the tokens in lower case**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bytes in lowercase column: 58999377\n",
      "CPU times: user 544 ms, sys: 12.1 ms, total: 557 ms\n",
      "Wall time: 555 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_tokens[\"Lowercase\"] = all_tokens[\"SearchTerm\"].map(lambda token: token.lower())\n",
    "print(\"bytes in lowercase column: \" + str(sys.getsizeof(all_tokens[\"Lowercase\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creates a new dataframe containing unique tokens and their number of occurances**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bytes in unique_tokens dataframe: 3789720\n",
      "CPU times: user 146 ms, sys: 157 µs, total: 146 ms\n",
      "Wall time: 144 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "unique_tokens = all_tokens[\"Lowercase\"].value_counts().to_frame()\n",
    "unique_tokens = unique_tokens.reset_index()\n",
    "unique_tokens.columns = [\"SearchTerm\", \"Occurances\"]\n",
    "print(\"bytes in unique_tokens dataframe: \" + str(sys.getsizeof(unique_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating an output csv file from the database of the unique tokens and counts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tokens.to_csv('/home/harleys/pandas_all_tokens_SP.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example results of tokens and their number of occurances**: In this cell several entries from the unique_tokens dataframe are printed out to illustrate what the csv file looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SearchTerm</th>\n",
       "      <th>Occurances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>159314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the</td>\n",
       "      <td>24874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you</td>\n",
       "      <td>22813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>to</td>\n",
       "      <td>19510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i</td>\n",
       "      <td>17280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  SearchTerm  Occurances\n",
       "0                 159314\n",
       "1        the       24874\n",
       "2        you       22813\n",
       "3         to       19510\n",
       "4          i       17280"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_tokens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creates a new dataframe containing only the alphabetic tokens and their number of occurances**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bytes in unique_alpha_tokens dataframe: 1501615\n",
      "CPU times: user 26.5 ms, sys: 163 µs, total: 26.6 ms\n",
      "Wall time: 25.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "unique_alpha_tokens = unique_tokens[unique_tokens[\"SearchTerm\"].str.isalpha()]\n",
    "print(\"bytes in unique_alpha_tokens dataframe: \" + str(sys.getsizeof(unique_alpha_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spellchecking the alphabetic tokens and adding the possible correct spelling as a new column**: This code originally threw a SettingWithCopy warning, but I determined that the code acted as intended so I hid the warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bytes in dataframe columns with tokens and their spellchecked versions: 2399202\n",
      "CPU times: user 21min 53s, sys: 38.8 s, total: 22min 31s\n",
      "Wall time: 22min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spell = SpellChecker(distance=2)\n",
    "unique_alpha_tokens[\"CorrectSpelling\"] = unique_alpha_tokens[\"SearchTerm\"].map(lambda token: spell.correction(token))\n",
    "unique_alpha_tokens = unique_alpha_tokens[[\"SearchTerm\", \"CorrectSpelling\", \"Occurances\"]]\n",
    "unique_alpha_tokens = unique_alpha_tokens.reset_index(drop=True)\n",
    "print(\"bytes in dataframe columns with tokens and their spellchecked versions: \" + str(sys.getsizeof(unique_alpha_tokens[\"SearchTerm\"]) + sys.getsizeof(unique_alpha_tokens[\"CorrectSpelling\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating an output csv file from the dataframe of tokens and their possible correct spellings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_alpha_tokens.to_csv('/home/harleys/pandas_tokens_and_correct_spellings_SP.csv', index=False, columns=[\"SearchTerm\", \"CorrectSpelling\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example results of tokens and their possible correct spellings**: In this cell several entries from the unique_alpha_tokens dataframe are printed out to illustrate what the csv file looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SearchTerm</th>\n",
       "      <th>CorrectSpelling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you</td>\n",
       "      <td>you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  SearchTerm CorrectSpelling\n",
       "0        the             the\n",
       "1        you             you\n",
       "2         to              to\n",
       "3          i               i\n",
       "4          a               a"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_alpha_tokens.iloc[0:5, 0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a final dataframe of unique spell checked tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bytes in unique_correct_spelling_tokens: 1065230\n",
      "CPU times: user 14.3 ms, sys: 3.95 ms, total: 18.3 ms\n",
      "Wall time: 17.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "unique_correct_spelling_tokens = pd.DataFrame(unique_alpha_tokens[\"CorrectSpelling\"].unique())\n",
    "unique_correct_spelling_tokens.columns = [\"SearchTerm\"]\n",
    "print(\"bytes in unique_correct_spelling_tokens: \" + str(sys.getsizeof(unique_correct_spelling_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a final dictionary of tokens**: In this cell, the final dictionary of spell checked tokens is created. If a token was misspelled and a correct spelling was found, then the number of occurances of the misspelled word is added to the number of occurances of the correctly spelled word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bytes in final_spelled_dict: 589928\n",
      "CPU times: user 127 ms, sys: 3.84 ms, total: 131 ms\n",
      "Wall time: 129 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "temp_df = unique_alpha_tokens.drop(columns=[\"SearchTerm\"])\n",
    "final_spelled_dict = {}\n",
    "row_index = 0\n",
    "for token in unique_correct_spelling_tokens[\"SearchTerm\"]:\n",
    "    final_spelled_dict[token] = 0;\n",
    "for token in temp_df[\"CorrectSpelling\"]:\n",
    "    final_spelled_dict[token] += temp_df.loc[row_index, \"Occurances\"]\n",
    "    row_index += 1\n",
    "print(\"bytes in final_spelled_dict: \" + str(sys.getsizeof(final_spelled_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating an output csv file from the dictionary of correctly spelled tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/harleys/pandas_correctly_spelled_tokens_SP.csv', 'w') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\"SearchToken\", \"Occurances\"])\n",
    "    writer.writeheader()\n",
    "    for key in final_spelled_dict.keys():\n",
    "        file.write(\"%s,%s\\n\"%(key,final_spelled_dict[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "- The dataset in this notebook is a file of lines from an episode of south park.\n",
    "\n",
    "- This file is about a third of the size of the search terms file that we were using previously. However, I don't think there will be many misspelled words in this file so I think it will be considerably quicker. I estimate that the cell running the spell check should complete in about 5 minutes (compared to 24 minutes of the searchterms file).\n",
    "\n",
    "- The actual runtime for this file was 22.5 minutes. \n",
    "\n",
    "- My hypothesis was pretty far off. I think that the average word size of this file was less which would mean there are more words per byte which caused me to underestimate. Also, there was greater diversity in the words in the file which also caused me to underestimate since I did not take that into account.\n",
    "\n",
    "- In conclusion, the size of a file can give you an idea of how long it can take a function to run, but you won't know for sure until you actually test it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
