{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Search Terms 2.0 Pandas\n",
    "## Stuart Harley\n",
    "\n",
    "**Introduction**: In lab 3 we created a list of the most popular search terms (tokens) for a given set of search queries.  As people are imperfect, they often misspell search terms, so we are using a spell checking library to remove and interpret misspellings to find the actual most popular terms (and not just the most popular and consistently spelled terms). The search terms come from Direct Supply's DSSI eProcurement system.\n",
    "\n",
    "In this lab, we are doing the same thing but we are using numpy and pandas to manipulate the data. This specific notebook uses Pandas.\n",
    "\n",
    "**Learning Outcomes**:\n",
    "- Data importing with Numpy and Pandas\n",
    "- Cleaning data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "import pattern.en\n",
    "import csv\n",
    "import time\n",
    "import sys\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "pd.options.mode.chained_assignment = None # hides a SettingWithCopy warning later on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function splits tokens at %20's and spaces**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_token(string):\n",
    "    return re.split('%20|\\s|,', string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing csv data into a pandas dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bytes in original_tokens: 68816387\n",
      "CPU times: user 643 ms, sys: 28 ms, total: 671 ms\n",
      "Wall time: 671 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "original_tokens = pd.read_csv('/home/harleys/searchTerms.csv', usecols=[\"SearchTerm\"], dtype=str)\n",
    "print(\"bytes in original_tokens: \" + str(sys.getsizeof(original_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creates a new dataframe containing all tokens**: (this splits the tokens at spaces, commas, or %20's.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bytes in all_tokens: 92425589\n",
      "CPU times: user 2.54 s, sys: 96.2 ms, total: 2.64 s\n",
      "Wall time: 2.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokens = original_tokens[\"SearchTerm\"].to_numpy()\n",
    "all_tokens = pd.DataFrame([elem for singleList in list(map(split_token, tokens)) for elem in singleList])\n",
    "all_tokens.columns = [\"SearchTerm\"]\n",
    "print(\"bytes in all_tokens: \" + str(sys.getsizeof(all_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adds a column in the dataframe containing the tokens in lower case**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bytes in lowercase column: 92382509\n",
      "CPU times: user 851 ms, sys: 16 ms, total: 867 ms\n",
      "Wall time: 865 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_tokens[\"Lowercase\"] = all_tokens[\"SearchTerm\"].map(lambda token: token.lower())\n",
    "print(\"bytes in lowercase column: \" + str(sys.getsizeof(all_tokens[\"Lowercase\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creates a new dataframe containing unique tokens and their number of occurances**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bytes in unique_tokens dataframe: 4417601\n",
      "CPU times: user 217 ms, sys: 8.49 ms, total: 226 ms\n",
      "Wall time: 223 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "unique_tokens = all_tokens[\"Lowercase\"].value_counts().to_frame()\n",
    "unique_tokens = unique_tokens.reset_index()\n",
    "unique_tokens.columns = [\"SearchTerm\", \"Occurances\"]\n",
    "print(\"bytes in unique_tokens dataframe: \" + str(sys.getsizeof(unique_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating an output csv file from the database of the unique tokens and counts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tokens.to_csv('/home/harleys/pandas_all_tokens.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example results of tokens and their number of occurances**: In this cell several entries from the unique_tokens dataframe are printed out to illustrate what the csv file looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SearchTerm</th>\n",
       "      <th>Occurances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chicken</td>\n",
       "      <td>19223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cream</td>\n",
       "      <td>16056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cheese</td>\n",
       "      <td>13955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>beef</td>\n",
       "      <td>13566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pie</td>\n",
       "      <td>11475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  SearchTerm  Occurances\n",
       "0    chicken       19223\n",
       "1      cream       16056\n",
       "2     cheese       13955\n",
       "3       beef       13566\n",
       "4        pie       11475"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_tokens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creates a new dataframe containing only the alphabetic tokens and their number of occurances**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bytes in unique_alpha_tokens dataframe: 861331\n",
      "CPU times: user 24.5 ms, sys: 31 µs, total: 24.6 ms\n",
      "Wall time: 23.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "unique_alpha_tokens = unique_tokens[unique_tokens[\"SearchTerm\"].str.isalpha()]\n",
    "print(\"bytes in unique_alpha_tokens dataframe: \" + str(sys.getsizeof(unique_alpha_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spellchecking the alphabetic tokens and adding the possible correct spelling as a new column**: This code originally threw a SettingWithCopy warning, but I determined that the code acted as intended so I hid the warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bytes in dataframe columns with tokens and their spellchecked versions: 1374150\n",
      "CPU times: user 23min 56s, sys: 27.5 s, total: 24min 23s\n",
      "Wall time: 24min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spell = SpellChecker(distance=2)\n",
    "unique_alpha_tokens[\"CorrectSpelling\"] = unique_alpha_tokens[\"SearchTerm\"].map(lambda token: spell.correction(token))\n",
    "unique_alpha_tokens = unique_alpha_tokens[[\"SearchTerm\", \"CorrectSpelling\", \"Occurances\"]]\n",
    "unique_alpha_tokens = unique_alpha_tokens.reset_index(drop=True)\n",
    "print(\"bytes in dataframe columns with tokens and their spellchecked versions: \" + str(sys.getsizeof(unique_alpha_tokens[\"SearchTerm\"]) + sys.getsizeof(unique_alpha_tokens[\"CorrectSpelling\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating an output csv file from the dataframe of tokens and their possible correct spellings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_alpha_tokens.to_csv('/home/harleys/pandas_tokens_and_correct_spellings.csv', index=False, columns=[\"SearchTerm\", \"CorrectSpelling\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example results of tokens and their possible correct spellings**: In this cell several entries from the unique_alpha_tokens dataframe are printed out to illustrate what the csv file looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SearchTerm</th>\n",
       "      <th>CorrectSpelling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chicken</td>\n",
       "      <td>chicken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cream</td>\n",
       "      <td>cream</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cheese</td>\n",
       "      <td>cheese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>beef</td>\n",
       "      <td>beef</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pie</td>\n",
       "      <td>pie</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  SearchTerm CorrectSpelling\n",
       "0    chicken         chicken\n",
       "1      cream           cream\n",
       "2     cheese          cheese\n",
       "3       beef            beef\n",
       "4        pie             pie"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_alpha_tokens.iloc[0:5, 0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a final dataframe of unique spell checked tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bytes in unique_correct_spelling_tokens: 536352\n",
      "CPU times: user 10.6 ms, sys: 50 µs, total: 10.7 ms\n",
      "Wall time: 10.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "unique_correct_spelling_tokens = pd.DataFrame(unique_alpha_tokens[\"CorrectSpelling\"].unique())\n",
    "unique_correct_spelling_tokens.columns = [\"SearchTerm\"]\n",
    "print(\"bytes in unique_correct_spelling_tokens: \" + str(sys.getsizeof(unique_correct_spelling_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a final dictionary of tokens**: In this cell, the final dictionary of spell checked tokens is created. If a token was misspelled and a correct spelling was found, then the number of occurances of the misspelled word is added to the number of occurances of the correctly spelled word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bytes in final_spelled_dict: 295008\n",
      "CPU times: user 76.1 ms, sys: 49 µs, total: 76.1 ms\n",
      "Wall time: 74.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "temp_df = unique_alpha_tokens.drop(columns=[\"SearchTerm\"])\n",
    "final_spelled_dict = {}\n",
    "row_index = 0\n",
    "for token in unique_correct_spelling_tokens[\"SearchTerm\"]:\n",
    "    final_spelled_dict[token] = 0;\n",
    "for token in temp_df[\"CorrectSpelling\"]:\n",
    "    final_spelled_dict[token] += temp_df.loc[row_index, \"Occurances\"]\n",
    "    row_index += 1\n",
    "print(\"bytes in final_spelled_dict: \" + str(sys.getsizeof(final_spelled_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating an output csv file from the dictionary of correctly spelled tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/harleys/pandas_correctly_spelled_tokens.csv', 'w') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\"SearchToken\", \"Occurances\"])\n",
    "    writer.writeheader()\n",
    "    for key in final_spelled_dict.keys():\n",
    "        file.write(\"%s,%s\\n\"%(key,final_spelled_dict[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "- In terms of space complexity, it appears that the numpy and pandas data structures use more bytes than simple lists and dictionaries. I believe this is because the numpy and pandas structures keep track of more information than a regular list. Also, numpy is not optimized for strings.\n",
    "\n",
    "- In terms of time complexity, the numpy and pandas structures took longer than the regular structures. I believe this is because numpy is optimized for numbers, not strings. And they are both not really designed to change the lengths of data structures. In terms of the spellchecking cell, they all took about the same amount of time to run.\n",
    "\n",
    "- In terms of performance, I think that it could be effective to use numpy for a set length of numbers. And to use pandas for a set length of strings or numbers where you want to operate on these datasets without really changing the length much.\n",
    "\n",
    "- In terms of usability, it can be annoying to do things in pandas and numpy. They have very powerful methods which if you understand them well, can definitely help a lot. However, if you are new to them, it can be very difficult to get the output you are looking for especially because many of the methods have a large number of optional parameters which can be confusing to understand."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
